from spacy import load
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation
nlp = load("en_core_web_sm")

def summarize(text,  per_sent_limit=0.2):
    """
    Summarizes a given text utilizing sentence similarity and  ranking.
    
    Args:
        text (str): The text that requires summarization.
         per_sent_limit (float): The proportion of sentences to be included in the summary.

    Returns:
         str: The resulting summarized text.
    """
    doc  = nlp(text)
    sentences =  [sent.text.strip() for sent in doc.sents if sent.text.strip()]
    sentence_vectors = []
    for sent in sentences:
         tokens = [token.text.lower() for token in nlp(sent) if  token.text.lower() not in STOP_WORDS and token.text not in punctuation]
         if tokens:
            sentence_vector =  np.mean([nlp.vocab[token].vector for token in tokens if token in nlp.vocab],  axis=0)
            sentence_vectors.append(sentence_vector)
    similarity_matrix = cosine_similarity(sentence_vectors)
    sentence_scores = similarity_matrix.sum(axis=1)
    ranked_sentences = [sentences[i] for i in  sentence_scores.argsort()[::-1]]
    num_sentences =  max(1, int(len(ranked_sentences) * per_sent_limit))
    summary_sentences =  ranked_sentences[:num_sentences]
    summary = "  ".join(summary_sentences)
    return summary
text = """
Your input text  goes here. It can be any length, and the summarizer will attempt to distill the most  important sentences based on their content and relevance. The algorithm works by calculating the similarities between the sentences in  the text.
"""
summary = summarize(text)
print("Summary:\n", summary)